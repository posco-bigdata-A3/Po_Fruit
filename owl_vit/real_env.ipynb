{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ef6907e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting real_env.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile real_env.py\n",
    "\n",
    "import os\n",
    "import dataclasses\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "WS_CROP_X = (0, 300)\n",
    "WS_CROP_Y = (0, 300)\n",
    "\n",
    "@dataclass\n",
    "class EnvState:\n",
    "    color_im: np.ndarray\n",
    "    depth_im: np.ndarray\n",
    "    objects: Dict[str, np.ndarray]  # 2D 좌표 (xmin, ymin, xmax, ymax)\n",
    "\n",
    "class RealEnv():\n",
    "    def __init__(self, bin_cam, task: str, all_objects: List[str], task_objects: List[str], output_name: str = None):\n",
    "        self.bin_cam = bin_cam\n",
    "        self.task = task\n",
    "        self.all_objects = all_objects\n",
    "        self.task_objects = task_objects\n",
    "        if output_name is None:\n",
    "            self.output_name = f\"real_world/outputs/{self.task}/\"\n",
    "        else:\n",
    "            self.output_name = f\"real_world/outputs/{output_name}/\"\n",
    "        os.makedirs(self.output_name, exist_ok=True)\n",
    "\n",
    "        self.robot_name = 'Bob'\n",
    "        self.human_name = 'Alice'\n",
    "\n",
    "        # Load OWL-ViT model\n",
    "        self.model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "        self.processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "\n",
    "        self.timestep = 0\n",
    "\n",
    "    def get_obs(self, save=False) -> EnvState:\n",
    "        # kinect.py 파일에서 color_im, depth_im 정보 받아옴.\n",
    "        color_im, depth_im = self.bin_cam.get_camera_data()\n",
    "        ws_color_im = color_im[WS_CROP_X[0]:WS_CROP_X[1], WS_CROP_Y[0]:WS_CROP_Y[1]]\n",
    "        ws_depth_im = depth_im[WS_CROP_X[0]:WS_CROP_X[1], WS_CROP_Y[0]:WS_CROP_Y[1]]\n",
    "\n",
    "        image = Image.fromarray(ws_color_im)\n",
    "        text = self.all_objects\n",
    "\n",
    "        # 객체 탐지 수행 (processor, 모델링)\n",
    "        inputs = self.processor(text=[text], images=image, return_tensors=\"pt\")\n",
    "        outputs = self.model(**inputs)\n",
    "\n",
    "        # 결과 처리\n",
    "        target_sizes = torch.Tensor([image.size[::-1]])\n",
    "        results = self.processor.post_process(outputs=outputs, target_sizes=target_sizes)\n",
    "        pred_scores = results[0][\"scores\"].detach().numpy()\n",
    "        pred_labels = results[0][\"labels\"].detach().numpy()\n",
    "        pred_boxes = results[0][\"boxes\"].detach().numpy()\n",
    "\n",
    "        objects = {}\n",
    "        for label, box in zip(pred_labels, pred_boxes):\n",
    "            if pred_scores[label] >= 0.5:  # 임계값 설정\n",
    "                objects[text[label]] = box\n",
    "\n",
    "        # 3D 좌표 계산\n",
    "        object_coords = []\n",
    "        for label, box in objects.items():\n",
    "            xmin, ymin, xmax, ymax = map(int, box)\n",
    "            center_x = (xmin + xmax) // 2\n",
    "            center_y = (ymin + ymax) // 2\n",
    "\n",
    "            depth = ws_depth_im[center_y, center_x]\n",
    "            if depth == 0:\n",
    "                continue\n",
    "\n",
    "            point_x = (center_x - self.bin_cam.intr.ppx) * depth / self.bin_cam.intr.fx\n",
    "            point_y = (center_y - self.bin_cam.intr.ppy) * depth / self.bin_cam.intr.fy\n",
    "            point_z = depth\n",
    "\n",
    "            object_coords.append({\"label\": label, \"coords\": (point_x, point_y, point_z)})\n",
    "\n",
    "        self.timestep += 1\n",
    "        if save:\n",
    "            image.save(f\"{self.output_name}/img_{self.timestep}.png\")\n",
    "\n",
    "        # 3D 좌표 출력\n",
    "        for obj in object_coords:\n",
    "            print(f\"Label: {obj['label']}, 3D Coordinates: {obj['coords']}\")\n",
    "\n",
    "        obs = EnvState(\n",
    "            color_im=color_im,\n",
    "            depth_im=depth_im,\n",
    "            objects=objects,\n",
    "        )\n",
    "        return obs\n",
    "\n",
    "    def plot_preds(self, color_im, objects, save=False, show=True):\n",
    "        fig, ax = plt.subplots(figsize=(12, 12 * color_im.shape[0] / color_im.shape[1]))\n",
    "        ax.imshow(color_im)\n",
    "        colors = sns.color_palette('muted', len(objects))\n",
    "        for label, c in zip(objects, colors):\n",
    "            (xmin, ymin, xmax, ymax) = map(int, objects[label])\n",
    "            ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color=c, linewidth=3))\n",
    "            if label in self.task_objects:\n",
    "                ax.text(xmin-30, ymax+15, label, fontsize=22, bbox=dict(facecolor='white', alpha=0.8))\n",
    "            else:\n",
    "                ax.text(xmin, ymin-10, label, fontsize=22, bbox=dict(facecolor='white', alpha=0.8))\n",
    "        plt.axis('off')\n",
    "        fig.tight_layout()\n",
    "\n",
    "        if show:\n",
    "            plt.show()\n",
    "        if save:\n",
    "            fig.savefig(f\"{self.output_name}/pred_{self.timestep}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c7c346-9cb9-44fd-8ef4-c210c2b66530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aec9854-a52a-4e62-873a-d2d71ddfcec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "966668e0-13ca-4cef-a586-c3323fcd5a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting real_env.py\n"
     ]
    }
   ],
   "source": [
    "# 다현언니 코드 기반 잘돌아감!\n",
    "\n",
    "\n",
    "%%writefile real_env.py\n",
    "\n",
    "#real_env.py 코드 파일입니다.\n",
    "import os\n",
    "import dataclasses\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "WS_CROP_X = (0, 300)\n",
    "WS_CROP_Y = (0, 300)\n",
    "\n",
    "# 환경 상태를 저장할 데이터 클래스 정의\n",
    "@dataclass\n",
    "class EnvState:\n",
    "    color_im: np.ndarray\n",
    "    depth_im: np.ndarray\n",
    "    objects: Dict[str, np.ndarray] # 2d 좌표. (xmin, ymin, xmax, ymax)로 구성되어 객체를 감싸는 사각형.\n",
    "    # ex) { \"apple\": np.array([50, 30, 200, 150]), \"banana\": np.array([120, 60, 250, 180]) }\n",
    "\n",
    "class RealEnv():\n",
    "    def __init__(self, bin_cam, task: str, all_objects: List[str], task_objects: List[str], output_name: str = None):\n",
    "        self.bin_cam = bin_cam\n",
    "        self.task = task\n",
    "        self.all_objects = all_objects\n",
    "        self.task_objects = task_objects\n",
    "        if output_name is None: #output_name 디폴트는 None. 따로 설정 안 해주면, self.task 로 디렉토리 이름 설정\n",
    "            self.output_name = f\"real_world/outputs/{self.task}/\"\n",
    "        else:\n",
    "            self.output_name = f\"real_world/outputs/{output_name}/\"\n",
    "        os.makedirs(self.output_name, exist_ok=True) # 출력 디렉토리 생성\n",
    "\n",
    "        self.robot_name = 'Bob' #로봇 이름: Bob\n",
    "        self.human_name = 'Alice' #사람 이름: Alice\n",
    "\n",
    "        # Load OWL-ViT model\n",
    "        self.model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\") # OWL-ViT 모델을 로드\n",
    "        self.processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\") # 이미지와 텍스트 데이터를 처리하기 위한 프로세서를 로드\n",
    "\n",
    "        self.timestep = 0\n",
    "\n",
    "    def get_obs(self, save=False) -> EnvState:\n",
    "        # kinect.py 파일에서 color_im, depth_im 정보 받아옴. \n",
    "        color_im, depth_im = self.bin_cam.get_camera_data()\n",
    "        ws_color_im = color_im[WS_CROP_X[0]:WS_CROP_X[1], WS_CROP_Y[0]:WS_CROP_Y[1]]\n",
    "        ws_depth_im = depth_im[WS_CROP_X[0]:WS_CROP_X[1], WS_CROP_Y[0]:WS_CROP_Y[1]]\n",
    "\n",
    "        image = Image.fromarray(ws_color_im)\n",
    "        text = self.all_objects\n",
    "\n",
    "        # Get max probability bounding boxes for each object label\n",
    "        # 객체 탐지 수행 (processor, 모델링)\n",
    "        inputs = self.processor(text=[text], images=image, return_tensors=\"pt\")\n",
    "        outputs = self.model(**inputs)\n",
    "\n",
    "        # 결과 처리\n",
    "        target_sizes = torch.Tensor([image.size[::-1]])\n",
    "        results = self.processor.post_process(outputs=outputs, target_sizes=target_sizes)\n",
    "        pred_scores = results[0][\"scores\"].detach().numpy()\n",
    "        pred_labels = results[0][\"labels\"].detach().numpy()\n",
    "        pred_boxes = results[0][\"boxes\"].detach().numpy()\n",
    "\n",
    "        # 객체 크기 비교 및 분류 - Origin from dahyeon\n",
    "        objects = {}\n",
    "        for label in np.unique(pred_labels):\n",
    "            max_score_idx = np.argmax(pred_scores[np.where(pred_labels == label)])\n",
    "            max_box = pred_boxes[np.where(pred_labels == label)][max_score_idx]\n",
    "            objects[text[label]] = max_box\n",
    "\n",
    "        self.timestep += 1\n",
    "        if save:\n",
    "            image.save(f\"{self.output_name}/img_{self.timestep}.png\")\n",
    "        obs = EnvState(\n",
    "            color_im=color_im,\n",
    "            depth_im=depth_im,\n",
    "            objects=objects,\n",
    "        )\n",
    "        return obs\n",
    "\n",
    "    def plot_preds(self, color_im, objects, save=False, show=True):\n",
    "        fig, ax = plt.subplots(figsize=(12, 12 * color_im.shape[0] / color_im.shape[1]))\n",
    "        ax.imshow(color_im)\n",
    "        colors = sns.color_palette('muted', len(objects))\n",
    "        for label, c in zip(objects, colors):\n",
    "            (xmin, ymin, xmax, ymax) = objects[label]\n",
    "            ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color=c, linewidth=3))\n",
    "            if label in self.task_objects:\n",
    "                ax.text(xmin-30, ymax+15, label, fontsize=22, bbox=dict(facecolor='white', alpha=0.8))\n",
    "            else:\n",
    "                ax.text(xmin, ymin-10, label, fontsize=22, bbox=dict(facecolor='white', alpha=0.8))\n",
    "        plt.axis('off')\n",
    "        fig.tight_layout()\n",
    "\n",
    "        if show:\n",
    "            plt.show()\n",
    "        if save:\n",
    "            fig.savefig(f\"{self.output_name}/pred_{self.timestep}.png\")\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ad4e43-65d3-4fa7-8a7a-65dcac3d0fff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch171",
   "language": "python",
   "name": "pytorch171"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
